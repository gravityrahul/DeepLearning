{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_RealWorldUseCases.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gravityrahul/DeepLearning/blob/master/05_RealWorldUseCases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axDnreUGlBlP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5cec9dcf-ce70-4966-883d-ed728fa6434c"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "!pip install picklable_itertools\n",
        "!pip install fuel\n",
        "!pip install foolbox\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "PROJECT_DIR = \"/content/gdrive/My Drive/kdd/\"\n",
        "import sys,os\n",
        "import numpy as np\n",
        "sys.path.append(PROJECT_DIR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Collecting picklable_itertools\n",
            "  Downloading https://files.pythonhosted.org/packages/75/bc/cda9191f0c92960ede6aa95e364443965246385faf62775cc30749931c2c/picklable-itertools-0.1.1.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from picklable_itertools) (1.15.0)\n",
            "Building wheels for collected packages: picklable-itertools\n",
            "  Building wheel for picklable-itertools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for picklable-itertools: filename=picklable_itertools-0.1.1-cp36-none-any.whl size=15591 sha256=568de2c594e6ac35cee7b62642605fa1ddc24b15fa1e64aa6f6182922f8e8c6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/dd/e2/ec30ef7c475e1d9fb966735984ba05f8710c67d7de5358c326\n",
            "Successfully built picklable-itertools\n",
            "Installing collected packages: picklable-itertools\n",
            "Successfully installed picklable-itertools-0.1.1\n",
            "Collecting fuel\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c2/b5fb651c90e908f79769b7dd3643982b6a9b1bac9449b8ab16f72612d4f5/fuel-0.2.0.tar.gz (184kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fuel) (1.15.0)\n",
            "Requirement already satisfied: picklable_itertools in /usr/local/lib/python3.6/dist-packages (from fuel) (0.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fuel) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from fuel) (2.10.0)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from fuel) (3.4.4)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from fuel) (3.38.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fuel) (19.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fuel) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fuel) (2.23.0)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->fuel) (2.7.1)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->fuel) (2.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2020.6.20)\n",
            "Building wheels for collected packages: fuel\n",
            "  Building wheel for fuel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuel: filename=fuel-0.2.0-cp36-cp36m-linux_x86_64.whl size=352763 sha256=da4da44ff96b931244b8fc3483fe8deb864ab381bdd8a1dfc7363c5b1bfe18cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/ff/1b/f3708731cf77a6d759cd12e68c0a27700c299fbe2bffd34fa3\n",
            "Successfully built fuel\n",
            "Installing collected packages: fuel\n",
            "Successfully installed fuel-0.2.0\n",
            "Collecting foolbox\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/47/a7714672c8cdc79b1f9251e12122890e51ed6c6ca30716a855e9c036d2d2/foolbox-3.0.4-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 3.4MB/s \n",
            "\u001b[?25hCollecting eagerpy==0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/4c/13ed2aba954c111ea0aff7e75a5e3c95b533504ba24718f7ea18b036c440/eagerpy-0.27.0-py3-none-any.whl\n",
            "Collecting GitPython>=3.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/1e/a45320cab182bf1c8656107b3d4c042e659742822fc6bff150d769a984dd/GitPython-3.1.7-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 16.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from foolbox) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.6/dist-packages (from foolbox) (3.7.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from foolbox) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from foolbox) (49.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.4MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Installing collected packages: eagerpy, smmap, gitdb, GitPython, foolbox\n",
            "Successfully installed GitPython-3.1.7 eagerpy-0.27.0 foolbox-3.0.4 gitdb-4.0.5 smmap-3.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqOTe04fThJ",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCZBE1UiDtPP",
        "colab_type": "text"
      },
      "source": [
        "# Structural Health Monitoring \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgj6Zm-3OQ3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b379e35d-4d18-4127-c5fc-1411c2269ccf"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/5_RealWordUseCases/\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1rlrF04igwAzYBcIFq1KjWCtpj5XV5PU2/Colab Notebooks/5_RealWordUseCases\n",
            "/content/gdrive/.shortcut-targets-by-id/1rlrF04igwAzYBcIFq1KjWCtpj5XV5PU2/Colab Notebooks/5_RealWordUseCases\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEFW_OsN0BRu",
        "colab_type": "text"
      },
      "source": [
        "## Sydney Harbour Bridge : Autoencoder based on  FFT Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtYl7MdZ0AeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "b24ade3c-2ee6-4e6c-fd4a-348a0f74d1df"
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import random\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# import tensorflow as tf\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "def create_model(input_dim, encoding_dim, hidden_dim):\n",
        "\t# create model\n",
        "    autoencoder = Sequential()\n",
        "    autoencoder.add(encoding_dim, activation='relu', input_shape=input_dim)\n",
        "    autoencoder.add(hidden_dim, activation='relu')\n",
        "    autoencoder.add(encoding_dim, activation='relu')\n",
        "    autoencoder.add(input_dim, activation='sigmoid')\n",
        "\n",
        "    autoencoder.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "\t# return autoencoder\n",
        "\n",
        "\n",
        "def tuning_model(X):\n",
        "    # tuning deep auto encoder parameter\n",
        "    # X: healthy data\n",
        "\n",
        "\t# parameter ranges\n",
        "    nb_epoch_list = [50, 100, 200, 300]\n",
        "    batch_size_list = [32, 64, 128, 256, 512]\n",
        "    input_dim = X.shape[1]  # num of features\n",
        "    encoding_dim = 100  # int(input_dim / 2)\n",
        "    hidden_dim = 10  # int(encoding_dim / 2)\n",
        "    nStd = 3\n",
        "\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    # encoder = Dense(encoding_dim, activation='tanh', activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
        "    encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "    encoder = Dense(hidden_dim, activation='relu')(encoder)\n",
        "\n",
        "    # encoder = Dense(int(hidden_dim / 2), activation='relu')(encoder)\n",
        "    # decoder = Dense(hidden_dim, activation='relu')(encoder)\n",
        "\n",
        "    decoder = Dense(encoding_dim, activation='relu')(encoder)\n",
        "    decoder = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "    autoencoder.summary()\n",
        "\n",
        "    autoencoder.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    # # early stopping\n",
        "    EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
        "\n",
        "    best_mse = sys.maxsize\n",
        "    for i in range(len(nb_epoch_list)):\n",
        "        for j in range(len(batch_size_list)):\n",
        "            history = autoencoder.fit(X, X,\n",
        "                                      epochs=nb_epoch_list[i],\n",
        "                                      batch_size=batch_size_list[j],\n",
        "                                      shuffle=True,\n",
        "                                      validation_split=0.2,\n",
        "                                      # validation_data=(X_test, X_test),\n",
        "                                      verbose=0).history\n",
        "\n",
        "            # X_predicted = autoencoder.predict(X)\n",
        "            # mse = np.mean(np.mean(np.power(X - X_predicted, 2), axis=1))\n",
        "            mse = history['val_loss'][-1]\n",
        "            if mse < best_mse:\n",
        "                best_mse = mse\n",
        "                best_epoch = nb_epoch_list[i]\n",
        "                best_batch_size = batch_size_list[j]\n",
        "\n",
        "    return best_epoch, best_batch_size\n",
        "\n",
        "# #############################################################################\n",
        "# Main\n",
        "positive_label = -1\n",
        "nStd = 3 # for outlier threshold of AE\n",
        "nu = 0.05\n",
        "nTrials = 1\n",
        "\n",
        "fs_svm = np.zeros(nTrials)\n",
        "gm_svm = np.zeros(nTrials)\n",
        "auc_svm = np.zeros(nTrials)\n",
        "fs_ae = np.zeros(nTrials)\n",
        "gm_ae = np.zeros(nTrials)\n",
        "auc_ae = np.zeros(nTrials)\n",
        "\n",
        "\n",
        "\n",
        "file_name = (PROJECT_DIR + '/data/27-06-2019/SHM/SHM_2_joints_2_sensors_Aug12_Oct12_fft.csv')\n",
        "\n",
        "nTrains = 1000\n",
        "\n",
        "\n",
        "\n",
        "print(file_name)\n",
        "\n",
        "data_df = pd.read_csv(file_name, header = None, index_col= False)\n",
        "X = data_df.iloc[:,1:].values\n",
        "y = data_df.iloc[:,0].values\n",
        "if sum(y==0)>0:\n",
        "    y[y==1] = -1\n",
        "    y[y==0] = 1\n",
        "\n",
        "index_healthy = np.where(y == 1)[0]\n",
        "index_damage = np.where(y == -1)[0]\n",
        "\n",
        "plt.subplots()\n",
        "plt.plot(X[index_damage[random.randint(0,len(index_damage))]], label='A damaged event')\n",
        "plt.plot(X[index_healthy[random.randint(0,len(index_healthy))]], label='A healthy event')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Typical events')\n",
        "plt.ylabel('Feature values')\n",
        "plt.xlabel('Event sample')\n",
        "plt.show(block=False)\n",
        "\n",
        "for i in range(nTrials):\n",
        "\n",
        "    print('Running trial {} ............'.format(i+1))\n",
        "\n",
        "    # train, test split\n",
        "    # index = np.arange(len(index_healthy))\n",
        "    index = np.random.permutation(len(index_healthy))\n",
        "    index_train = index_healthy[index[:nTrains]]\n",
        "    index_test = np.concatenate((index_healthy[index[nTrains:]], index_damage))\n",
        "    X_train = X[index_train,:]\n",
        "    y_train = y[index_train]\n",
        "    X_test = X[index_test,:]\n",
        "    y_test = y[index_test]\n",
        "\n",
        "    # best_epoch, best_batch_size = tuning_model(X_train)\n",
        "\n",
        "    if i==0:\n",
        "        print('Train data: (n,d) = ', X_train.shape)\n",
        "        print('Test data: (n,d) = ', X_test.shape)\n",
        "        print('Test data: healthy = {}, damage = {}'.format(sum(y_test==1),sum(y_test==-1)))\n",
        "\n",
        "    # scaler = StandardScaler()  # data normalization\n",
        "    scaler = MinMaxScaler() # data normalization\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # OCSVM\n",
        "    model = OneClassSVM(nu=nu, kernel=\"rbf\", gamma = 'auto')\n",
        "    model.fit(X_train)\n",
        "    y_predicted = model.predict(X_test)\n",
        "    scores_svm = model.decision_function(X_test)\n",
        "\n",
        "    # confusion matrix\n",
        "    print('OCSVM results:')\n",
        "    # f1 score\n",
        "    fs_svm[i] = f1_score(y_test, y_predicted, pos_label = positive_label)\n",
        "    print(\"F1 score = {}\".format(fs_svm[i]))\n",
        "\n",
        "    # balanced accuracy\n",
        "    gm_svm[i] = balanced_accuracy_score(y_test, y_predicted)\n",
        "    print(\"G-mean = {}\".format(gm_svm[i]))\n",
        "\n",
        "    # AUC\n",
        "    auc_svm[i] = roc_auc_score(y_test, scores_svm)\n",
        "    print(\"AUC = {}\".format(auc_svm[i]))\n",
        "\n",
        "    conmat_svm = confusion_matrix(y_test, y_predicted)\n",
        "    print('Confusion matrix =\\n {}'.format(conmat_svm))\n",
        "\n",
        "    # auto encoder\n",
        "    nb_epoch = 50 #100\n",
        "    batch_size = 512# 128\n",
        "    input_dim = X_train.shape[1] #num of features\n",
        "    encoding_dim = 100 # int(input_dim / 2)\n",
        "    hidden_dim =  10 # int(encoding_dim / 2)\n",
        "    learning_rate = 1e-7\n",
        "\n",
        "    input_layer = Input(shape=(input_dim, ))\n",
        "    # encoder = Dense(encoding_dim, activation='tanh', activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
        "    encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "    encoder = Dense(hidden_dim, activation='relu')(encoder)\n",
        "\n",
        "    # encoder = Dense(int(hidden_dim / 2), activation='relu')(encoder)\n",
        "    # decoder = Dense(hidden_dim, activation='relu')(encoder)\n",
        "\n",
        "    decoder = Dense(encoding_dim, activation='relu')(encoder)\n",
        "    decoder = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "    autoencoder.summary()\n",
        "\n",
        "    autoencoder.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    # # early stopping\n",
        "    EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
        "\n",
        "    history = autoencoder.fit(X_train, X_train,\n",
        "                        epochs=nb_epoch,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        validation_split=0.2,\n",
        "                        # validation_data=(X_test, X_test),\n",
        "                        verbose=0).history\n",
        "\n",
        "    X_predicted_train = autoencoder.predict(X_train)\n",
        "    mse_train = np.mean(np.power(X_train - X_predicted_train, 2), axis=1)\n",
        "    threshold = np.mean(mse_train) + nStd*np.std(mse_train)\n",
        "\n",
        "    X_predicted = autoencoder.predict(X_test)\n",
        "    mse = np.mean(np.power(X_test - X_predicted, 2), axis=1)\n",
        "    y_predicted = np.ones(len(y_test))\n",
        "    y_predicted[np.where(mse > threshold)] = -1\n",
        "\n",
        "    # confusion matrix\n",
        "    print('Deep auto-encoder results:')\n",
        "    # f1 score\n",
        "    fs_ae[i] = f1_score(y_test, y_predicted, pos_label = positive_label)\n",
        "    print(\"F1 score = {}\".format(fs_ae[i]))\n",
        "\n",
        "    # balanced accuracy\n",
        "    gm_ae[i] = balanced_accuracy_score(y_test, y_predicted)\n",
        "    print(\"G-mean = {}\".format(gm_ae[i]))\n",
        "\n",
        "    scores_ae = max(mse) - mse\n",
        "    # scores_ae = mse\n",
        "    auc_ae[i] = roc_auc_score(y_test, scores_ae)\n",
        "    print(\"AUC = {}\".format(auc_ae[i]))\n",
        "\n",
        "    conmat_ae = confusion_matrix(y_test, y_predicted)\n",
        "    print('Confusion matrix =\\n {}'.format(conmat_ae))\n",
        "\n",
        "\n",
        "    # # robust deep auto encoder\n",
        "    if i == 0:\n",
        "        # learning curve of AE\n",
        "        plt.subplots()\n",
        "        plt.plot(history['loss'], linewidth=2, label='Train')\n",
        "        plt.plot(history['val_loss'], linewidth=2, label='Test')\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.title('Model loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.show(block=False)\n",
        "\n",
        "        # decision values\n",
        "        x = np.arange(len(y_test))\n",
        "        plt.subplots()\n",
        "        plt.plot(x[np.where(y_test==1)], scores_svm[np.where(y_test==1)], '.b', label='Healthy')\n",
        "        plt.plot(x[np.where(y_test==-1)], scores_svm[np.where(y_test==-1)], 'xr', label='Damaged')\n",
        "        plt.plot(x, np.zeros(len(x)), 'k--')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.title('OCSVM')\n",
        "        plt.ylabel('Decision values')\n",
        "        plt.xlabel('Test samples')\n",
        "        plt.show(block=False)\n",
        "\n",
        "        plt.subplots()\n",
        "        plt.plot(x[np.where(y_test==1)], mse[np.where(y_test==1)], '.b', label='Healthy')\n",
        "        plt.plot(x[np.where(y_test==-1)], mse[np.where(y_test==-1)], 'xr', label='Damaged')\n",
        "        plt.plot(x, [threshold] * len(x), 'k--')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.title('Deep autoencoder')\n",
        "        plt.ylabel('Decision values')\n",
        "        plt.xlabel('Test samples')\n",
        "        plt.show(block=False)\n",
        "\n",
        "print('\\n---------------------------------------')\n",
        "print('OCSVM results (nTrials = {}):'.format(nTrials))\n",
        "print(\"Average F1 score = {}\".format(np.mean(fs_svm)))\n",
        "print(\"Average g-mean = {}\".format(np.mean(gm_svm)))\n",
        "print(\"Average AUC = {}\".format(np.mean(auc_svm)))\n",
        "print('A confusion matrix =\\n {}'.format(conmat_svm))\n",
        "\n",
        "print('\\nDeep auto-encoder results (nTrials = {}):'.format(nTrials))\n",
        "print(\"Average F1 score = {}\".format(np.mean(fs_ae)))\n",
        "print(\"Average g-mean = {}\".format(np.mean(gm_ae)))\n",
        "print(\"Average AUC = {}\".format(np.mean(auc_ae)))\n",
        "print('A confusion matrix =\\n {}'.format(conmat_ae))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-855c7c685bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPROJECT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/27-06-2019/SHM/SHM_2_joints_2_sensors_Aug12_Oct12_fft.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mnTrains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PROJECT_DIR' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0-pJPqyPtrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvb8LVd1FWTy",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AkeqZBUPFXqB"
      },
      "source": [
        "## Sydney Harbour Bridge : Robust Autoencoder based on  FFT Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LA5E2d7FFXqE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "890a56ed-e70f-4528-a792-fc57016eadc1"
      },
      "source": [
        "## Obtaining the training and testing data\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "from src.models.RCAE import RCAE_AD\n",
        "import numpy as np \n",
        "from src.config import Configuration as Cfg\n",
        "\n",
        "DATASET = \"shb\"\n",
        "DATA_DIM= 900\n",
        "IMG_HGT =0\n",
        "IMG_WDT=0\n",
        "IMG_CHANNEL=0\n",
        "HIDDEN_LAYER_SIZE= 10\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/shm/RCAE/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/SHM/RCAE/\"\n",
        "\n",
        "\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "#RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n",
        "RANDOM_SEED = [42]\n",
        "AUC = []\n",
        "\n",
        "for seed in RANDOM_SEED:  \n",
        "  Cfg.seed = seed\n",
        "  Cfg.method = \"rcae\"\n",
        "  rcae = RCAE_AD(DATASET,DATA_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n",
        "  print('Train data: (n,d) = ', rcae.data._X_train.shape)\n",
        "  print('Test data: (n,d) = ', rcae.data._X_test.shape)\n",
        "  print('Test data: healthy = {}, damage = {}'.format(sum(rcae.data._y_test==0),sum(rcae.data._y_test==1)))\n",
        "  print(\"===========TRAINING AND PREDICTING WITH RAE============================\")\n",
        "  auc_roc = rcae.fit_and_predict()\n",
        "  print(\"========================================================================\")\n",
        "  AUC.append(auc_roc)\n",
        "  \n",
        "  \n",
        "print(\"===========TRAINING AND PREDICTING WITH RAE============================\")\n",
        "print(\"AUROC computed \", AUC)\n",
        "auc_roc_mean = np.mean(np.asarray(AUC))\n",
        "auc_roc_std = np.std(np.asarray(AUC))\n",
        "print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n",
        "print(\"========================================================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/one_class_neural_networks/src/models/RCAE.py:20: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RCAE.RESULT_PATH: /content/gdrive/My Drive/one_class_neural_networks//notebooks/OCNN-Results/\n",
            "INFO: The dataset is  shb\n",
            "[INFO ]:  Configuring experiment for Anomaly Detection [AD]\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "ES = No of Events recorded  per Sensor: 11867\n",
            "Normal ES = No of Normal Events recorded  per Sensor: ==? \n",
            "AbNormal ES = No of Abnormal Events recorded  per Sensor: ==?  Need to get a plot and separate the data.\n",
            "Total Events Captured (m):  35601\n",
            "Total Number of Features (d)  : 3000\n",
            "X : ( m X d) :  (35601, 3000)\n",
            "Considering 3 Features (X1,X2,X3) features: \n",
            "X1 : ( m X d) :  (20652, 1800)\n",
            "X2 : ( m X d) :  (14949, 1800)\n",
            "X_Anomalies (20652, 1800)\n",
            "X_Normal (14949, 1800)\n",
            "[INFO]: Train data: (n,d) =  (4983, 1800)\n",
            "[INFO]: Test data: (n,d) =  (21652, 1800)\n",
            "[INFO]: Test data: healthy = 1000, damage = 20652\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1800)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               180100    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1800)              181800    \n",
            "=================================================================\n",
            "Total params: 364,010\n",
            "Trainable params: 364,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[INFO:] Assertions of memory muted\n",
            "[INFO ]:  Configuring experiment for Anomaly Detection [AD]\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "ES = No of Events recorded  per Sensor: 11867\n",
            "Normal ES = No of Normal Events recorded  per Sensor: ==? \n",
            "AbNormal ES = No of Abnormal Events recorded  per Sensor: ==?  Need to get a plot and separate the data.\n",
            "Total Events Captured (m):  35601\n",
            "Total Number of Features (d)  : 3000\n",
            "X : ( m X d) :  (35601, 3000)\n",
            "Considering 3 Features (X1,X2,X3) features: \n",
            "X1 : ( m X d) :  (20652, 1800)\n",
            "X2 : ( m X d) :  (14949, 1800)\n",
            "X_Anomalies (20652, 1800)\n",
            "X_Normal (14949, 1800)\n",
            "[INFO]: Train data: (n,d) =  (4983, 1800)\n",
            "[INFO]: Test data: (n,d) =  (21652, 1800)\n",
            "[INFO]: Test data: healthy = 1000, damage = 20652\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 1800)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               180100    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1800)              181800    \n",
            "=================================================================\n",
            "Total params: 364,010\n",
            "Trainable params: 364,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train data: (n,d) =  (4983, 1800)\n",
            "Test data: (n,d) =  (21652, 1800)\n",
            "Test data: healthy = 0, damage = 1000\n",
            "===========TRAINING AND PREDICTING WITH RAE============================\n",
            "[INFO] Compiling RAE model...\n",
            "[INFO:] Shape of U, V (100, 10) (10, 100)\n",
            "[INFO:] Shape of U, V(Transpose) (100, 10) (100, 10)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "(lamda,Threshold) 0.6 0.3\n",
            "The type of b is ..., its len is  <class 'numpy.ndarray'> (4983, 1800) 1800\n",
            "Iteration NUmber is :  0\n",
            "NUmber of non zero elements  for N,lamda 2957 0.6\n",
            "The shape of N (4983, 1800)\n",
            "The minimum value of N  -0.45411315925982826\n",
            "The max value of N 0.59767986536026\n",
            "[INFO]: Robust Autoencoder  training completed !!!!\n",
            "The computed threshold is 0.004960667996579258\n",
            "Confusion matrix =\n",
            " [[15014  5638]\n",
            " [   31   969]]\n",
            "===================================\n",
            "========================================================================\n",
            "===========TRAINING AND PREDICTING WITH RAE============================\n",
            "AUROC computed  [[0.9888762831687002, 0.8411911365100708, 0.8479999031570792]]\n",
            "AUROC ===== 0.89268910761195 +/- 0.06807138107409584\n",
            "========================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2X6_PjqFjHu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XhzaB2JtFk17"
      },
      "source": [
        "## Sydney Harbour Bridge : Deep SVDD  based on  FFT Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YdfE2k8nFk1-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e3d2558-bf9b-458a-fcbe-94037a264185"
      },
      "source": [
        "## Obtaining the training and testing data\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from src.models.OneClass_SVDD import OneClass_SVDD\n",
        "from src.models.config import Configuration as Cfg\n",
        "\n",
        "DATASET = \"shb\"\n",
        "DATA_DIM= 900\n",
        "IMG_HGT =0\n",
        "IMG_WDT=0\n",
        "IMG_CHANNEL=0\n",
        "HIDDEN_LAYER_SIZE= 10\n",
        "\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/SHM/OC_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/SHM/OC_NN/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "#LOSS_FUNCTION = \"SOFT_BOUND_DEEP_SVDD\"\n",
        "LOSS_FUNCTION = \"ONE_CLASS_DEEP_SVDD\"\n",
        "#LOSS_FUNCTION = \"ONE_CLASS_NEURAL_NETWORK\"\n",
        "\n",
        "import os\n",
        "os.chdir(PROJECT_DIR)\n",
        "#RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n",
        "RANDOM_SEED = [42]\n",
        "AUC = []\n",
        "\n",
        "for seed in RANDOM_SEED:  \n",
        "  Cfg.seed = seed\n",
        "  ocnn = OneClass_SVDD(DATASET,LOSS_FUNCTION,DATA_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed) \n",
        "  print(\"[INFO:] Testing with ALL other  DIGITs  as anomalies\")\n",
        "  ocnn.fit()\n",
        "  print(\"==============PREDICTING THE LABELS ==============================\")\n",
        "  auc_score = ocnn.predict()\n",
        "  AUC.append(auc_score)\n",
        "\n",
        "print(\"===========TRAINING AND PREDICTING WITH OCSVDD============================\")\n",
        "print(\"AUROC computed \", AUC)\n",
        "auc_roc_mean = np.mean(np.asarray(AUC))\n",
        "auc_roc_std = np.std(np.asarray(AUC))\n",
        "print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n",
        "print(\"========================================================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/.shortcut-targets-by-id/1KAza7E-Y9p3BqCC00_Dv98LMkDu22sn_/one_class_neural_networks/src/models/OneClass_SVDD.py:21: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OneClass_SVDD.RESULT_PATH: /content/gdrive/My Drive/one_class_neural_networks//reports/figures/SHM/OC_NN/\n",
            "[INFO ]:  Configuring experiment for Anomaly Detection [AD]\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "ES = No of Events recorded  per Sensor: 11867\n",
            "Normal ES = No of Normal Events recorded  per Sensor: ==? \n",
            "AbNormal ES = No of Abnormal Events recorded  per Sensor: ==?  Need to get a plot and separate the data.\n",
            "Total Events Captured (m):  35601\n",
            "Total Number of Features (d)  : 3000\n",
            "X : ( m X d) :  (35601, 3000)\n",
            "Considering 3 Features (X1,X2,X3) features: \n",
            "X1 : ( m X d) :  (20652, 1800)\n",
            "X2 : ( m X d) :  (14949, 1800)\n",
            "X_Anomalies (20652, 1800)\n",
            "X_Normal (14949, 1800)\n",
            "[INFO]: Train data: (n,d) =  (4983, 1800)\n",
            "[INFO]: Test data: (n,d) =  (21652, 1800)\n",
            "[INFO]: Test data: healthy = 1000, damage = 20652\n",
            "Calling the data loader inside OneClassSVDD 4983\n",
            "[INFO:] Assertions of memory muted\n",
            "[INFO ]:  Configuring experiment for Anomaly Detection [AD]\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "ES = No of Events recorded  per Sensor: 11867\n",
            "Normal ES = No of Normal Events recorded  per Sensor: ==? \n",
            "AbNormal ES = No of Abnormal Events recorded  per Sensor: ==?  Need to get a plot and separate the data.\n",
            "Total Events Captured (m):  35601\n",
            "Total Number of Features (d)  : 3000\n",
            "X : ( m X d) :  (35601, 3000)\n",
            "Considering 3 Features (X1,X2,X3) features: \n",
            "X1 : ( m X d) :  (20652, 1800)\n",
            "X2 : ( m X d) :  (14949, 1800)\n",
            "X_Anomalies (20652, 1800)\n",
            "X_Normal (14949, 1800)\n",
            "[INFO]: Train data: (n,d) =  (4983, 1800)\n",
            "[INFO]: Test data: (n,d) =  (21652, 1800)\n",
            "[INFO]: Test data: healthy = 1000, damage = 20652\n",
            "[INFO:] Testing with ALL other  DIGITs  as anomalies\n",
            " [INFO:]  The shape  of  trainX data ---- (4983, 1800)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "[INFO] Compiling AE model...\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1800)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               180100    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1800)              181800    \n",
            "=================================================================\n",
            "Total params: 364,010\n",
            "Trainable params: 364,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[INFO:] The shape of X used to train CAE (4983, 1800)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "[INFO:] Obtained the initial representations of input using pretrained weights \n",
            "[INFO:] Initializing c and Radius R value...\n",
            "[INFO:] Radius (R)  initialized. 111.92109916882265\n",
            "[INFO:] Hypersphere Loss function.....\n",
            "[INFO:] \n",
            " Model compiled and fit Initial Radius Value... 4.035200119018555\n",
            "The computed threshold is 4.820245683193207\n",
            "==============PREDICTING THE LABELS ==============================\n",
            "[INFO:] One Class Deep SVDD Algorithm\n",
            "Confusion matrix =\n",
            " [[20186   466]\n",
            " [   40   960]]\n",
            "===================================\n",
            "[INFO:]  AUROC Oneclass SVDD (Hypersphere).... 0.9946718477629286\n",
            "===================================\n",
            "===========TRAINING AND PREDICTING WITH OCSVDD============================\n",
            "AUROC computed  [[0.9946718477629286, 0.7914262159934048, 0.9687177997288399]]\n",
            "AUROC ===== 0.9182719544950578 +/- 0.09031716036413472\n",
            "========================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmAI7_p5FvSf",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W6KaRKYHFv4m"
      },
      "source": [
        "## Sydney Harbour Bridge : Soft Deep SVDD based on  FFT Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tLnKron_Fv4q",
        "colab": {}
      },
      "source": [
        "## Obtaining the training and testing data\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from src.models.OneClass_SVDD import OneClass_SVDD\n",
        "from src.models.config import Configuration as Cfg\n",
        "\n",
        "DATASET = \"shb\"\n",
        "DATA_DIM= 900\n",
        "IMG_HGT =0\n",
        "IMG_WDT=0\n",
        "IMG_CHANNEL=0\n",
        "HIDDEN_LAYER_SIZE= 10\n",
        "\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/SHM/OC_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/SHM/OC_NN/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "LOSS_FUNCTION = \"SOFT_BOUND_DEEP_SVDD\"\n",
        "#LOSS_FUNCTION = \"ONE_CLASS_DEEP_SVDD\"\n",
        "#LOSS_FUNCTION = \"ONE_CLASS_NEURAL_NETWORK\"\n",
        "\n",
        "import os\n",
        "os.chdir(PROJECT_DIR)\n",
        "#RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n",
        "RANDOM_SEED = [42]\n",
        "AUC = []\n",
        "\n",
        "for seed in RANDOM_SEED:  \n",
        "  Cfg.seed = seed\n",
        "  ocnn = OneClass_SVDD(DATASET,LOSS_FUNCTION,DATA_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed) \n",
        "  print(\"[INFO:] Testing with ALL other  DIGITs  as anomalies\")\n",
        "  ocnn.fit()\n",
        "  print(\"==============PREDICTING THE LABELS ==============================\")\n",
        "  auc_score = ocnn.predict()\n",
        "  AUC.append(auc_score)\n",
        "\n",
        "print(\"===========TRAINING AND PREDICTING WITH OCSVDD============================\")\n",
        "print(\"AUROC computed \", AUC)\n",
        "auc_roc_mean = np.mean(np.asarray(AUC))\n",
        "auc_roc_std = np.std(np.asarray(AUC))\n",
        "print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n",
        "print(\"========================================================================\")#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpxprl_KF3o3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nNe3NcPJF4sC"
      },
      "source": [
        "## Sydney Harbour Bridge : One Class Neural Networks (OCNN) based on  FFT Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bokYkAjpF4sF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d5acb83-dc99-48b7-c22e-b300c6f723ac"
      },
      "source": [
        "## Obtaining the training and testing data\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from src.models.OneClass_SVDD import OneClass_SVDD\n",
        "from src.models.config import Configuration as Cfg\n",
        "\n",
        "DATASET = \"shb\"\n",
        "DATA_DIM= 900\n",
        "IMG_HGT =0\n",
        "IMG_WDT=0\n",
        "IMG_CHANNEL=0\n",
        "HIDDEN_LAYER_SIZE= 10\n",
        "\n",
        "MODEL_SAVE_PATH = PROJECT_DIR + \"/models/SHM/OC_NN/\"\n",
        "REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/SHM/OC_NN/\"\n",
        "PRETRAINED_WT_PATH = \"\"\n",
        "\n",
        "#LOSS_FUNCTION = \"SOFT_BOUND_DEEP_SVDD\"\n",
        "#LOSS_FUNCTION = \"ONE_CLASS_DEEP_SVDD\"\n",
        "LOSS_FUNCTION = \"ONE_CLASS_NEURAL_NETWORK\"\n",
        "\n",
        "import os\n",
        "os.chdir(PROJECT_DIR)\n",
        "#RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n",
        "RANDOM_SEED = [42]\n",
        "AUC = []\n",
        "\n",
        "for seed in RANDOM_SEED:  \n",
        "  Cfg.seed = seed\n",
        "  ocnn = OneClass_SVDD(DATASET,LOSS_FUNCTION,DATA_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed) \n",
        "  print(\"[INFO:] Testing with ALL other  DIGITs  as anomalies\")\n",
        "  ocnn.fit()\n",
        "  print(\"==============PREDICTING THE LABELS ==============================\")\n",
        "  auc_score = ocnn.predict()\n",
        "  AUC.append(auc_score)\n",
        "\n",
        "print(\"===========TRAINING AND PREDICTING WITH OCSVDD============================\")\n",
        "print(\"AUROC computed \", AUC)\n",
        "auc_roc_mean = np.mean(np.asarray(AUC))\n",
        "auc_roc_std = np.std(np.asarray(AUC))\n",
        "print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n",
        "print(\"========================================================================\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/one_class_neural_networks/src/models/OneClass_SVDD.py:21: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OneClass_SVDD.RESULT_PATH: /content/gdrive/My Drive/one_class_neural_networks//reports/figures/SHM/OC_NN/\n",
            "[INFO ]:  Configuring experiment for Anomaly Detection [AD]\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "ES = No of Events recorded  per Sensor: 11867\n",
            "Normal ES = No of Normal Events recorded  per Sensor: ==? \n",
            "AbNormal ES = No of Abnormal Events recorded  per Sensor: ==?  Need to get a plot and separate the data.\n",
            "Total Events Captured (m):  35601\n",
            "Total Number of Features (d)  : 3000\n",
            "X : ( m X d) :  (35601, 3000)\n",
            "Considering 3 Features (X1,X2,X3) features: \n",
            "X1 : ( m X d) :  (20652, 1800)\n",
            "X2 : ( m X d) :  (14949, 1800)\n",
            "X_Anomalies (20652, 1800)\n",
            "X_Normal (14949, 1800)\n",
            "[INFO]: Train data: (n,d) =  (4983, 1800)\n",
            "[INFO]: Test data: (n,d) =  (21652, 1800)\n",
            "[INFO]: Test data: healthy = 1000, damage = 20652\n",
            "Calling the data loader inside OneClassSVDD 4983\n",
            "[INFO:] Assertions of memory muted\n",
            "[INFO ]:  Configuring experiment for Anomaly Detection [AD]\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "[INFO:] Loading SHB RAW data begin from path...\n",
            "/content/gdrive/My Drive/one_class_neural_networks/data/shm/SHM_2_joints_6_sensors_5_features_Aug12_Oct12_merged.mat\n",
            "ES = No of Events recorded  per Sensor: 11867\n",
            "Normal ES = No of Normal Events recorded  per Sensor: ==? \n",
            "AbNormal ES = No of Abnormal Events recorded  per Sensor: ==?  Need to get a plot and separate the data.\n",
            "Total Events Captured (m):  35601\n",
            "Total Number of Features (d)  : 3000\n",
            "X : ( m X d) :  (35601, 3000)\n",
            "Considering 3 Features (X1,X2,X3) features: \n",
            "X1 : ( m X d) :  (20652, 1800)\n",
            "X2 : ( m X d) :  (14949, 1800)\n",
            "X_Anomalies (20652, 1800)\n",
            "X_Normal (14949, 1800)\n",
            "[INFO]: Train data: (n,d) =  (4983, 1800)\n",
            "[INFO]: Test data: (n,d) =  (21652, 1800)\n",
            "[INFO]: Test data: healthy = 1000, damage = 20652\n",
            "[INFO:] Testing with ALL other  DIGITs  as anomalies\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "[INFO] Compiling AE model...\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1800)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               180100    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1800)              181800    \n",
            "=================================================================\n",
            "Total params: 364,010\n",
            "Trainable params: 364,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[INFO:] The shape of X used to train CAE (4983, 1800)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "[INFO:] Obtained the initial representations of input using pretrained weights \n",
            "[INFO:] Initializing c and Radius R value...\n",
            "[INFO:] Radius (R)  initialized. 72.29943622362481\n",
            "model ocnn summary\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 100)               180100    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                100       \n",
            "=================================================================\n",
            "Total params: 181,210\n",
            "Trainable params: 181,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[INFO:] Hyperplane Loss function.....\n",
            "Tensor(\"loss_1/dense_5_loss/custom_hinge/Maximum_1:0\", shape=(?, 10), dtype=float32)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "[INFO:] \n",
            " Model compiled  Outside : Updated Radius Value... 0.0010099224798816222\n",
            "The computed threshold is 0.287763848900795\n",
            "==============PREDICTING THE LABELS ==============================\n",
            "[INFO:] Final Rvar used is  0.0010099224798816222\n",
            "[INFO:] One Class Neural Network Algorithm\n",
            "Confusion matrix =\n",
            " [[15948  4704]\n",
            " [   39   961]]\n",
            "===================================\n",
            "[INFO:]  AUROC: Oneclass Neural Network (OCNN) .... 0.9572774307573115\n",
            "===================================\n",
            "===========TRAINING AND PREDICTING WITH OCSVDD============================\n",
            "AUROC computed  [[0.9572774307573115, 0.8705477769589781, 0.8666127251597908]]\n",
            "AUROC ===== 0.8981459776253602 +/- 0.041843101544441735\n",
            "========================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8mGk1bmGl0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}